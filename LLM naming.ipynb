{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc102334-8113-4070-b4de-1ab470536543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in /databricks/python3/lib/python3.11/site-packages (0.27.1)\n",
      "Requirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub) (2023.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub) (6.0)\n",
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub) (2023.7.22)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub\n",
    "\n",
    "from huggingface_hub import login\n",
    "# token=\"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\"\n",
    "\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca27b85-263e-49c6-a8b9-7685e15f294c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from pyspark.sql.functions import collect_list\n",
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket('gcs-dsci-fryou-fy-dev-prd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61831e16-781d-416d-9d4c-eaa7a78eb1ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "version = 1\n",
    "path = \"gs://gcs-dsci-fryou-fy-dev-prd/catalog_vqvae/vqvae_catalog_embeddings_2025-03-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "358e9978-34a4-4957-b0e0-7a68fcc5ba00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461e1870-0a19-480a-b355-9cb3c2ea441a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path = \"gs://gcs-dsci-fryou-fy-dev-prd/catalog_vqvae/kmeans_50000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "230306a8-7221-4583-8d32-8c3895b83f65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "taxonomy = spark.sql(\"\"\"select distinct catalog_id, sscat_id, scat_id, sscat from gold.product_info\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3bfdac7-e4d6-4c70-96f0-68ad65d2cff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_catalog_50 = spark.read.parquet(output_path)\n",
    "result_50 = search_catalog_50.join(df, on=\"search_catalog_embedding\", how=\"inner\")\n",
    "intersected_result_50 = result_50.join(\n",
    "    taxonomy, \n",
    "    on=\"catalog_id\", \n",
    "    how=\"inner\"\n",
    ").select(\"cluster\", \"catalog_id\", \"sscat_id\", \"sscat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d77876c4-3786-4580-8033-bbffc715e2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_summary = spark.sql(\"select distinct catalog_id, summary from ds_silver.product_image_captions\")\n",
    "catalog_summary = catalog_summary.join(intersected_result_50, on=\"catalog_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d76301f-71ad-4406-976b-8abdf5782d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_grouped = catalog_summary.groupBy(\"cluster\").agg(collect_list(\"summary\").alias(\"summary_list\"), collect_list(\"sscat\").alias(\"sscat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6442dfe7-fd5a-4147-b705-44fe3d339754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path = \"gs://gcs-dsci-fryou-fy-dev-prd/catalog_vqvae/kmeans_50000_wordmap\"\n",
    "df_cluster_wordmap = spark.read.parquet(f\"{output_path}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74157ab-243c-4684-af35-033116fb9c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_llm = df_grouped.join(df_cluster_wordmap.dropDuplicates([\"cluster\"]), on=\"cluster\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf2dc24-b7c7-4fe4-8b4b-73fc20f61682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_llm_1 = df_llm.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f143bef5-24ca-427e-84da-e1477750b572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:07:59.770088: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-10 12:07:59.834834: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "class SimpleCategoryGenerator:\n",
    "    def __init__(self, model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", token=None):\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\",\n",
    "            token=token\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def generate_categories(self, product_description, category_hints=None, features=None):\n",
    "        category_hints = category_hints or []\n",
    "        features = features or []\n",
    "            \n",
    "        category_str = \", \".join(category_hints[:5]) if category_hints else \"Unknown\"\n",
    "        features_str = \", \".join(features[:5]) if features else \"Unknown\"\n",
    "        \n",
    "        # prompt\n",
    "        prompt = f\"\"\"Create a JSON object for this e-commerce product with proper categorization:\n",
    "\n",
    "            Product: {product_description[:250]}\n",
    "            Categories: {category_str}\n",
    "            Features: {features_str}\n",
    "\n",
    "            Return JSON with these hierarchical fields:\n",
    "            - \"Category\": Top-level department or main product group (e.g., Electronics, Clothing, Home & Garden, Beauty, Sports & Outdoors)\n",
    "            - \"Sub-Category\": Specific type within that category (must be more specific than Category)\n",
    "            - \"Cluster Name\": Descriptive name (5-7 words) highlighting key selling points\n",
    "\n",
    "            IMPORTANT: Category and Sub-Category must be different. If product is \"Men's Running Shoes\", \n",
    "            Category might be \"Footwear\" or \"Sports Equipment\" and Sub-Category would be \"Athletic Shoes\" or \"Running Shoes\".\n",
    "\n",
    "            Examples of good Category → Sub-Category pairs:\n",
    "            • Clothing → Men's T-Shirts\n",
    "            • Kitchen → Coffee Makers\n",
    "            • Electronics → Bluetooth Speakers\n",
    "            • Beauty → Hair Styling Tools\n",
    "            • Furniture → Office Chairs\n",
    "\n",
    "            JSON:\n",
    "            {{\n",
    "            \"Category\": \"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.5,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "        new_tokens = outputs[0][prompt_length:]\n",
    "        generated_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        full_json = '{\\n  \"Category\": ' + generated_text\n",
    "        \n",
    "        if not self._is_complete_json(full_json):\n",
    "            full_json = self._complete_json(full_json, category_hints, features)\n",
    "        \n",
    "        try:\n",
    "            return json.loads(full_json)\n",
    "        except json.JSONDecodeError:\n",
    "            return self._extract_json_fields(full_json)\n",
    "    \n",
    "    def _is_complete_json(self, json_str):\n",
    "        \"\"\"Check if the JSON contains all required fields and is properly closed\"\"\"\n",
    "        return ('\"Category\"' in json_str and \n",
    "                '\"Sub-Category\"' in json_str and \n",
    "                '\"Cluster Name\"' in json_str and \n",
    "                json_str.strip().endswith(\"}\"))\n",
    "    \n",
    "    def _complete_json(self, partial_json, category_hints, features):\n",
    "        \"\"\"Complete partial JSON with missing fields\"\"\"\n",
    "        if '\"Sub-Category\"' not in partial_json:\n",
    "            partial_json += ',\\n  \"Sub-Category\": '\n",
    "            if len(category_hints) > 1:\n",
    "                partial_json += f'\"{category_hints[1]}\"'\n",
    "            elif len(category_hints) > 0:\n",
    "                partial_json += f'\"{category_hints[0]} Products\"'  # Added 'Products' to differentiate\n",
    "            else:\n",
    "                partial_json += '\"Various\"'\n",
    "        \n",
    "        if '\"Cluster Name\"' not in partial_json:\n",
    "            partial_json += ',\\n  \"Cluster Name\": '\n",
    "            if features and len(features) >= 2:\n",
    "                partial_json += f'\"{features[0]} {features[1]} Collection\"'  # Added 'Collection'\n",
    "            elif features and len(features) == 1:\n",
    "                partial_json += f'\"Premium {features[0]} Products\"'  # Added 'Premium' and 'Products'\n",
    "            else:\n",
    "                partial_json += '\"Quality Product Collection\"'\n",
    "        \n",
    "        if not partial_json.strip().endswith(\"}\"):\n",
    "            partial_json += '\\n}'\n",
    "            \n",
    "        return partial_json\n",
    "    \n",
    "    def _extract_json_fields(self, text):\n",
    "        \"\"\"Extract fields using regex when JSON parsing fails\"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        for field in [\"Category\", \"Sub-Category\", \"Cluster Name\"]:\n",
    "            match = re.search(f'\"{field}\":\\\\s*\"([^\"]+)\"', text)\n",
    "            if match:\n",
    "                result[field] = match.group(1)\n",
    "        \n",
    "        if \"Category\" not in result and \"Sub-Category\" not in result:\n",
    "            result[\"Category\"] = \"General Merchandise\"\n",
    "            result[\"Sub-Category\"] = \"Various Products\"\n",
    "        elif \"Category\" in result and \"Sub-Category\" not in result:\n",
    "            result[\"Sub-Category\"] = f\"{result['Category']} Products\"\n",
    "        elif \"Category\" not in result and \"Sub-Category\" in result:\n",
    "            words = result[\"Sub-Category\"].split()\n",
    "            if words:\n",
    "                result[\"Category\"] = words[0]\n",
    "            else:\n",
    "                result[\"Category\"] = \"General Merchandise\"\n",
    "                \n",
    "        if \"Cluster Name\" not in result:\n",
    "            if \"Sub-Category\" in result:\n",
    "                result[\"Cluster Name\"] = f\"Premium {result['Sub-Category']} Collection\"\n",
    "            else:\n",
    "                result[\"Cluster Name\"] = \"Quality Product Collection\"\n",
    "                \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b1462bd-bb61-4331-978f-2e4a85163089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: Arshimeesho\n"
     ]
    }
   ],
   "source": [
    "# Authentication with Hugging Face\n",
    "token = \"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\"\n",
    "api = HfApi()\n",
    "user = api.whoami(token=token)\n",
    "print(f\"Authenticated as: {user['name']}\")\n",
    "\n",
    "def process_batch_with_llm(iterator):\n",
    "    \"\"\"Process a partition of data with the LLM.\"\"\"\n",
    "    import torch\n",
    "    import json\n",
    "\n",
    "    generator = SimpleCategoryGenerator(token=token)\n",
    "    \n",
    "    results = []\n",
    "    batch_size = 5  \n",
    "    current_batch = []\n",
    "    \n",
    "    for row in iterator:\n",
    "        current_batch.append(row)\n",
    "        \n",
    "        if len(current_batch) >= batch_size:\n",
    "            processed_batch = process_mini_batch(current_batch, generator)\n",
    "            results.extend(processed_batch)\n",
    "            current_batch = []\n",
    "    \n",
    "    if current_batch:\n",
    "        processed_batch = process_mini_batch(current_batch, generator)\n",
    "        results.extend(processed_batch)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_mini_batch(batch, generator):\n",
    "    \"\"\"Process a mini-batch of rows\"\"\"\n",
    "    results = []\n",
    "    for row in batch:\n",
    "        try:\n",
    "            summaries = \"\\n\".join(row.summary_list[:5]) if hasattr(row, 'summary_list') and row.summary_list else \"\"\n",
    "            \n",
    "            sscat_list = []\n",
    "            if hasattr(row, 'sscat'):\n",
    "                if isinstance(row.sscat, list):\n",
    "                    sscat_list = row.sscat\n",
    "                elif row.sscat:\n",
    "                    sscat_list = [row.sscat]\n",
    "            \n",
    "            tokens_list = []\n",
    "            if hasattr(row, 'tokens'):\n",
    "                if isinstance(row.tokens, list):\n",
    "                    tokens_list = row.tokens\n",
    "                elif row.tokens:\n",
    "                    tokens_list = [row.tokens]\n",
    "            \n",
    "            result = generator.generate_categories(\n",
    "                product_description=summaries,\n",
    "                category_hints=sscat_list,\n",
    "                features=tokens_list\n",
    "            )\n",
    "            \n",
    "            new_row = row.asDict()\n",
    "            new_row[\"category\"] = result.get(\"Category\", \"\")\n",
    "            new_row[\"sub_category\"] = result.get(\"Sub-Category\", \"\")\n",
    "            new_row[\"cluster_name\"] = result.get(\"Cluster Name\", \"\")\n",
    "            \n",
    "            for field in [\"summary_list\", \"tokens\", \"sscat\"]:\n",
    "                if field in new_row:\n",
    "                    del new_row[field]\n",
    "                    \n",
    "            results.append(Row(**new_row))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {e}\")\n",
    "            new_row = row.asDict()\n",
    "            for field in [\"summary_list\", \"tokens\", \"sscat\"]:\n",
    "                if field in new_row:\n",
    "                    del new_row[field]\n",
    "            new_row[\"category\"] = \"\"\n",
    "            new_row[\"sub_category\"] = \"\"\n",
    "            new_row[\"cluster_name\"] = \"\"\n",
    "            results.append(Row(**new_row))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "101dd4b2-f9dd-420c-a263-823cdc93d1f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_dataframe(df, chunk_size=10):\n",
    "    total_count = df.count()\n",
    "    print(f\"Total rows to process: {total_count}\")\n",
    "    \n",
    "    if total_count <= chunk_size:\n",
    "        print(f\"Processing all {total_count} rows at once\")\n",
    "        return df.rdd.mapPartitions(process_batch_with_llm).toDF()\n",
    "    \n",
    "    print(f\"Processing in chunks of {chunk_size} rows\")\n",
    "    results = []\n",
    "    \n",
    "    num_chunks = (total_count + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        print(f\"Processing chunk {i+1} of {num_chunks}\")\n",
    "        chunk_df = df.limit(chunk_size).offset(i * chunk_size)\n",
    "        \n",
    "        chunk_df = chunk_df.repartition(4) \n",
    "        \n",
    "        result_df = chunk_df.rdd.mapPartitions(process_batch_with_llm).toDF()\n",
    "        results.append(result_df)\n",
    "        print(f\"Chunk {i+1} completed\")\n",
    "    \n",
    "    final_df = spark.createDataFrame([], schema=results[0].schema)\n",
    "    for df in results:\n",
    "        final_df = final_df.union(df)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "df_llm_1 = df_llm.limit(10)\n",
    "df_final = process_dataframe(df_llm_1, chunk_size=5)\n",
    "df_final.cache()\n",
    "print(\"Processing complete. Displaying results...\")\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c7e9bc8-ba26-43eb-9ebb-9705333b1969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LLM naming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
